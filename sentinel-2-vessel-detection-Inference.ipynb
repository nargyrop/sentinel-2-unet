{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: default\n",
    "mathjax: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-2 Vessel Semantic Segmentation  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "This is my first all-around ML project. It probably wasn't the best choice for a starter, but I like to get my hands dirty and this is a project that I have worked on in the past, but without the ML aspect.\n",
    "\n",
    "I started off by trying my luck with Random Forests and XGBoosting, both of which gave me some nice results. The problem at the time was the data conditioning, which I wasn't doing sufficiently well.\n",
    "\n",
    "After that, I decided to try my luck with Deep Learning and I already knew about the capabilities of the UNET architecture for semantic segmentation. I used a read-made model which I found on github, and that was basically my starting point. After a lot of failures, some of which took less and some took more time, I finally gained a better understanding of what was and what wasn't working to my benefit.\n",
    "\n",
    "As with many Deep Learning projects, I have now come to a halt because the data is insufficient to achieve increased accuracy. The model performs adequately, but there's definitely room for improvements. Unfortunately, a lack of free time keeps me from creating more data (which is a painfully slow and tedious task!).\n",
    "\n",
    "This model was trained on about 1787 positive samples and 615 negative samples. The biggest issue I faced was class imbalance. It's easy to observe that the number of pixels in a Sentinel-2 scene (10980x10980 pixels) that correspond to vessels at any given time, are very small compared to the whole scene. As a matter of fact the exact number of pixels in each class are shown in the following table:\n",
    "\n",
    "| Class | Number of Samples (Tiles) | Number of Samples (Pixels) | Contribution (%) |\n",
    "| :-----------: | :-----------: | :-----------: | :-----------: |\n",
    "| 0 | 615 | 9,702,986 | 98.62 |\n",
    "| 1 | 1787 | 135,606 | 1.38 |\n",
    "\n",
    "The above table shows that the contribution of the positive class to the total number of samples is just about 1.4%, while the corresponding figure for the negative class is about 98.6%! This imbalance will skew the training metrics, which in turn affects the learning capabilities of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "As I mentioned before, I used the UNET architecture that utilizes CNNs, which is a very powerful architecture for semantic segmentation and it was proposed <sup>[1]</sup> for use in biomedical image segmentation. The UNET model comprises of a compression path (left-hand side of the image), that uses MaxPooling layers and compresses the image 16x by the time it reaches the bottom of the **_U_** , and an expansive path (right-hand side of the image), that upsamples the downsampled image 16x by the time it has reached the top-most layer. That way, the model gains the ability to effectively learn both the finer and the coarser details of the images.\n",
    "\n",
    "![UNET](./assets/images/unet-architecture.png)\n",
    "\n",
    "If you plan to implement this architecture, bear in mind that the input image size needs to be a power of 2, with the smaller size being that of 64x64 pixels.\n",
    "\n",
    "[1]: https://arxiv.org/abs/1505.04597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
    "### Data Augmentation, Loss Functions, Optimizers\n",
    "\n",
    "#### Data Augmentation\n",
    "\n",
    "When it comes to images, data augmentation techniques include, but are not limited to, scaling, shearing, rotations, mirroring, etc. In my case, I knew right off the bat that I didn't want to choose a data augmentation technique that would alter/resample my training data, as this would mean that the spectral information would be altered as well.\n",
    "\n",
    "For this reason, I went with 3 rotations ($90^o$, $180^o$ and $270^o$) and 2 flips, 1 along the X-axis and 1 along the Y-axis. That way, no resampling needed to be used, thus maintaining the original information of the pixels.\n",
    "\n",
    "#### Loss functions\n",
    "\n",
    "For the loss function, I decided to look further that the `binary crossentropy loss` and utilize some function that's better suited for segmentation problems. Then, came the Jaccard Index, which led me to find a loss function implemented using the Dice coefficient. The Dice coefficient is given by: \n",
    "\n",
    "<div style=\"text-align: center\">$D=\\frac{2\\times \\sum_{i=1}^{n} p_i g_i}{\\sum_{i=1}^{n} p_i^{2} + \\sum_{i=1}^{n} g_i^{2}}=\\frac{2\\times Intersection}{Union},\\\\ where\\ p_i: \\ Prediction\\ and\\ g_i:\\ Ground\\ truth$</div>\n",
    "\n",
    "This loss function gave me better results -considering the vast class imbalance- and subsequent skewness of the metrics.\n",
    "\n",
    "#### Optimizers\n",
    "\n",
    "Regarding the optimizer, I first went with Stochastic Gradient Descent with Nesterov momentum enabled, because I thought that if I could get the parameters right, I would get really good results. I quickly found out that this wasn't the case, and I believe that the class imbalance had a significant effect to that end. Then, I turned to using the Adam optimizer, which needed no tuning to start giving me some legit results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The Sentinel-2 satellites offer 13 bands in total at spatial resolution ranging from 10m to 60m. Sentinel-2 currently offer 2 levels of data, Level 1C and Level 2A. In this project, I used L1C products. Spatial resolution is a deciding factor -since if I selected the coarser bands the model wouldn't be able to detect small vessels-, but so is the spectral information. For this project, I decided that the bands which would give me a good trade-off between spectral information and spatial resolution are the ones shown in the table below:\n",
    "\n",
    "| Band | Central Wavelength (nm) | Spatial Resolution (m) | Used |\n",
    "| :----------- | :-----------: | :-----------: | :-----------: | \n",
    "| B01 - Coastal Aerosol | 492.3 | 60 | &#9746; |\n",
    "| B02 - Blue | 492.3 | 10 | &#9745; |\n",
    "| B03 - Green | 559.4 | 10 | &#9745; |\n",
    "| B04 - Red | 664.8 | 10 | &#9745; |\n",
    "| B05 - Vegetation red edge | 704.0 | 20 | &#9746; |\n",
    "| B06 - Vegetation red edge | 739.9 | 20 | &#9746; |\n",
    "| B07 - Vegetation red edge | 781.3 | 20 | &#9746; |\n",
    "| B08 - NIR | 832.8 | 10 | &#9745; |\n",
    "| B8A - Narrow NIR | 864.4 | 20 | &#9745; |\n",
    "| B09 - Water vapour | 944.1 | 60 | &#9746; |\n",
    "| B10 - SWIR Cirrus | 1375.2 | 60 | &#9746; |\n",
    "| B11 - SWIR-1 | 1612.1 | 20 | &#9745; |\n",
    "| B12 - SWIR-2 | 2194.1 | 20 | &#9745; |\n",
    "\n",
    "I discarded all the bands with 60m pixel size, because I needed to be able to detect smaller vessels (e.g. leisure or fishing boats), as well as large container vessels, and the 60m would probably decrease my chance to achieve it. Similarly, if I decided to use only the 10m bands (Blue, Green, Red, NIR), I would have the advantage of higher spatial resolution, but I would lose all the information in the other 3 bands, which are also captured in distinct and non-overlapping parts of the light spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "To gain a better understanding of the data, I plotted distribution and violin plots for both classes and for each band.\n",
    "\n",
    "Distribution Plot | Violin Plot\n",
    ":-:|:-:\n",
    "![B02 distribution](./assets/images/B02_distribution.png) | ![B02 violin](./assets/images/B02_violin.png)\n",
    "![B03 distribution](./assets/images/B03_distribution.png) | ![B03 violin](./assets/images/B03_violin.png)\n",
    "![B04 distribution](./assets/images/B04_distribution.png) | ![B04 violin](./assets/images/B04_violin.png)\n",
    "![B08 distribution](./assets/images/B08_distribution.png) | ![B08 violin](./assets/images/B08_violin.png)\n",
    "![B08A distribution](./assets/images/B8A_distribution.png) | ![B8A violin](./assets/images/B8A_violin.png)\n",
    "![B11 distribution](./assets/images/B11_distribution.png) | ![B11 violin](./assets/images/B11_violin.png)\n",
    "![B12 distribution](./assets/images/B12_distribution.png) | ![B12 violin](./assets/images/B12_violin.png)\n",
    "\n",
    "From these plots we can see that in certain cases (B02, B03), there is some spectral overlap between the two classes. This means that vessels and non-vessels can be detected in similar reflectance value ranges, which calls for a highly non-linear approach, perfectly suited for Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook, along with the model, can be found in [this](https://github.com/nargyrop/sentinel-2-unet.git) repo.\n",
    "Feel free to download a Sentinel-2 scene and try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fun part\n",
    "\n",
    "Let's start by importing all the necessary modules. The `pyna` module is a library of functions I have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=(DeprecationWarning, FutureWarning))\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import xmltodict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import folium\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from pyna.rasterlib import Raster\n",
    "from pyproj import Proj, transform\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_coordinates(coord_list, epsg_in):\n",
    "    \"\"\"\n",
    "    Function to reproject coordinate tuples to a target CRS using its EPSG code.\n",
    "    \"\"\"\n",
    "    \n",
    "    in_proj = Proj(init=f\"epsg:{epsg_in}\")\n",
    "    out_proj = Proj(init=\"epsg:4326\")\n",
    "    \n",
    "    # To store list of transformed coordinates\n",
    "    coord_transf = []\n",
    "    \n",
    "    for coord in coord_list:\n",
    "        coord_transf.append(transform(in_proj,out_proj,coord[0],coord[1]))\n",
    "    \n",
    "    return coord_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameters(xml_dir, xml_fname):\n",
    "    \"\"\"\n",
    "    Function to load model weights and preprocessing parameters.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(xml_dir, xml_fname)) as fd:\n",
    "        doc = xmltodict.parse(fd.read())\n",
    "    \n",
    "    # Not all of the bands might be needed. This is also specified on the filename.\n",
    "    bands = list(map(int, doc['training_parameters']['bands'].split(',')))\n",
    "    ndvi_cr = doc['training_parameters']['ndvi']\n",
    "    \n",
    "    ndvi_cr = 'True' == True\n",
    "    \n",
    "    padding = int(doc['training_parameters']['padding'])\n",
    "    max_vals = list(map(float, doc['training_parameters']['max_values'].split(',')))  # For normalization\n",
    "\n",
    "    return bands, ndvi_cr, padding, max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro = Raster()  # Instance of my rasterlib library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is were I'll initialize some required variables, for example the location of files and the required bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziploc = \"/home/nikos/Downloads/S2A_MSIL1C_20200525T100031_N0209_R122_T33TUL_20200525T120308.zip\"  # target zip file\n",
    "req_bands = (\"B02\", \"B03\", \"B04\", \"B08\", \"B8A\", \"B11\", \"B12\")  # required bands\n",
    "file_extn = \"jp2\"  # raster extension\n",
    "pixel_size = 10  # target pixel size\n",
    "\n",
    "xml_mod_dir = './trained_models'  # Directory were the model is stored\n",
    "xml_fname = 'unet_2020-03-30_11-00_12345.xml'  # XML that includes preprocessing parameters\n",
    "model_fname = xml_fname.replace('xml', 'json')  # Model\n",
    "weights_fname = xml_fname.replace('xml', 'h5')  # Model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all the required bands are loaded into arrays straight from the zip file. That way we're saving time and disk space. Along with the arrays, we'll get some more data, like the transformation tuple, the projection string and the EPSG code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required bands from S-2 zip file\n",
    "band_dict, dict_keys = ro.load_from_zip(ziploc, req_bands, file_extn)\n",
    "dict_keys.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll make sure each band's pixel size is the same as the target one. For this model, we need 10m pixels. We'll also amend the transformation tuple of each band to match the correct pixel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample all the bands to the target pixel size\n",
    "for band in band_dict.keys():\n",
    "    if band_dict[band][1][1] != pixel_size:\n",
    "        band_dict[band][0] = ro.resample_array(band_dict[band][0],\n",
    "                                               in_pix=band_dict[band][1][1],\n",
    "                                               out_pix=pixel_size)\n",
    "        transf = list(band_dict[band][1])\n",
    "        transf[1] = pixel_size\n",
    "        transf[-1] = -pixel_size\n",
    "        band_dict[band][1] = tuple(transf)\n",
    "transf = band_dict[dict_keys[0]][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the bands have matching pixel sizes, we'll stack them to a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_arr = np.dstack([band_dict[band][0] for band in dict_keys])  # Stack all bands to a single array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model required tiles of size (64, 64, 7 (bands)), we'll tile the stack. The output will be an array of shape (171, 171, 1, 64, 64, 7). This function utilizes a very nice and computationally efficient numpy function, called `stride_tricks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_merged_arr = ro.get_tiles(merged_arr, ksize=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the inference, we need to load the model and some preprocessing parameters (normalization values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model training parameters\n",
    "band_list, ndvi_cr, padding, max_vals = get_model_parameters(xml_mod_dir, xml_fname)\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(os.path.join(xml_mod_dir, model_fname), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(os.path.join(xml_mod_dir, weights_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll preprocess the image and have it ready for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tiled_merged_arr.shape[-1] + 1 * ndvi_cr == len(band_list):\n",
    "    tiled_merged_arr = tiled_merged_arr[:, :, :, :, :, band_list]\n",
    "tiled_merged_arr = np.squeeze(tiled_merged_arr)\n",
    "\n",
    "# There is the option of also including the NDVI in the bands\n",
    "if ndvi_cr:\n",
    "    ndvi = np.divide(tiled_merged_arr[:, :, :, :, 3] - tiled_merged_arr[:, :, :, :, 2],\n",
    "                     tiled_merged_arr[:, :, :, :, 3] + tiled_merged_arr[:, :, :, :, 2])\n",
    "    ndvi = np.nan_to_num(ndvi[:, :, :, :, np.newaxis])\n",
    "    tiled_merged_arr = np.concatenate([tiled_merged_arr, ndvi], axis=4)\n",
    "    ndvi = None  # Remove from memory\n",
    "\n",
    "# This is where the normalization happens\n",
    "tiled_norm_arr = tiled_merged_arr.astype(np.float32)\n",
    "for band in range(tiled_merged_arr.shape[-1]):\n",
    "    tiled_norm_arr[:, :, :, :, band] /= max_vals[band]\n",
    "\n",
    "# We can also apply some padding to each tile, in order to reduce edge effects\n",
    "tiled_norm_pad = np.pad(tiled_norm_arr,\n",
    "                        ((0, 0), (0, 0), (padding, padding), (padding, padding), (0, 0)), mode='symmetric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to run the inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_merged_arr, tiled_norm_arr = None, None  # Remove some variables from memory\n",
    "\n",
    "# Get the number of tiles in each direction, tile size in each direction and number of bands\n",
    "ntilesy, ntilesx, sy, sx, nbands = tiled_norm_pad.shape\n",
    "\n",
    "# Reshape the array to stack all tiles along the y-axis\n",
    "tiled_reshaped = tiled_norm_pad.reshape(ntilesy * ntilesx, sy + 2 * padding, sx + 2 * padding, nbands)\n",
    "tiled_norm_pad = None  # You know what this does by now\n",
    "\n",
    "# Run the inference\n",
    "predictions = model.predict(tiled_reshaped)\n",
    "\n",
    "# Remove the padding if it's been applied\n",
    "if padding:\n",
    "    predictions = predictions[:, padding:-padding, padding:-padding, :]\n",
    "\n",
    "# Reshape back to match the original image\n",
    "outsl_res = predictions.reshape(ntilesy, ntilesx, sy, sx)\n",
    "out_arr = np.concatenate(np.concatenate(outsl_res, axis=1), axis=1)\n",
    "\n",
    "predictions, outsl_res, tiled_reshaped, model = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inference mask 3D and add alpha channel\n",
    "alpha_ch = np.zeros_like(out_arr)\n",
    "alpha_ch[out_arr > 0.5] = 255  # We'll make transparent all the pixels with less than 50% probability\n",
    "out_arr = np.dstack((out_arr * 255, np.zeros_like(out_arr), out_arr, alpha_ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the inference is done, we'll display the mask against the RGB image. The Sentinel-2 images are 16-bits. In order to correctly display them with Folium, we'll convert them to 8-bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_arr = ro.rgb16to8(merged_arr[:, :, [2, 1, 0]])\n",
    "merged_arr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the geographical extents for the RGB image\n",
    "extents_ls_rgb = [(transf[0], transf[3] + rgb_arr.shape[0] * transf[5]),\n",
    "                  (transf[0] + rgb_arr.shape[1] * transf[1], transf[3])]\n",
    "\n",
    "# And for the inference mask\n",
    "extents_ls_inf = [(transf[0], transf[3] + out_arr.shape[0] * transf[5]),\n",
    "                  (transf[0] + out_arr.shape[1] * transf[1], transf[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg = band_dict[dict_keys[0]][3]  # get the EPSG from any of the bands that have been loaded\n",
    "\n",
    "transformed_ls_rgb = reproject_coordinates(extents_ls_rgb, epsg)  # Convert RGB image extents to WGS84\n",
    "min_lon_rgb, min_lat_rgb = transformed_ls_rgb[0]\n",
    "max_lon_rgb, max_lat_rgb = transformed_ls_rgb[1]\n",
    "\n",
    "transformed_ls_inf = reproject_coordinates(extents_ls_inf, epsg)  # Convert Inference mask extents to WGS84\n",
    "min_lon_inf, min_lat_inf = transformed_ls_inf[0]\n",
    "max_lon_inf, max_lat_inf = transformed_ls_inf[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done. We just need to create our map using Folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folium' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-745f5c01be86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Setup the map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m inf_map = folium.Map(location=(np.mean([min_lat_rgb, max_lat_rgb]),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                np.mean([min_lon_rgb, max_lon_rgb])),\n\u001b[1;32m      4\u001b[0m                      \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'folium' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup the map\n",
    "inf_map = folium.Map(location=(np.mean([min_lat_rgb, max_lat_rgb]),\n",
    "                               np.mean([min_lon_rgb, max_lon_rgb])),\n",
    "                     width=640,\n",
    "                     height=480,\n",
    "                     zoom_start=8)\n",
    "                     \n",
    "# First display the RGB image\n",
    "rgb_img = folium.raster_layers.ImageOverlay(\n",
    "                                            name='Image',\n",
    "                                            image=rgb_arr,\n",
    "                                            bounds=[[min_lat_rgb, min_lon_rgb],\n",
    "                                                    [max_lat_rgb, max_lon_rgb]],\n",
    "                                            opacity=1,\n",
    "                                            interactive=True,\n",
    "                                            cross_origin=False,\n",
    "                                            zindex=1,\n",
    "                                            )\n",
    "\n",
    "# Then, display the inference mask\n",
    "mask_img = folium.raster_layers.ImageOverlay(\n",
    "                                             name='Inference Mask',\n",
    "                                             image=out_arr,\n",
    "                                             bounds=[[min_lat_inf, min_lon_inf],\n",
    "                                                     [max_lat_inf, max_lon_inf]],\n",
    "                                             opacity=0.6,\n",
    "                                             interactive=True,\n",
    "                                             cross_origin=False,\n",
    "                                             zindex=1,\n",
    "                                             )\n",
    "\n",
    "# Add both to the map\n",
    "rgb_img.add_to(inf_map)\n",
    "mask_img.add_to(inf_map)\n",
    "\n",
    "# And also add LayerControl so that either can be turned on and off\n",
    "_ = folium.LayerControl().add_to(inf_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_map.save(\"test_map.html\")  # save to html in order to display it on a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, but not least..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(filename=\"test_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to view map in jupyter notebook\n",
    "inf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
